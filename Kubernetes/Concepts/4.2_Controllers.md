### ReplicaSet

pod ların replica larını pod hangi durumda ve zamand a olursa olsu düzenlemek için kullanılır.

example

```yml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3
```

```
kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
```

ckeck replica set

```
kubectl describe rs/frontend
```

ayrıca yaml olara da sonucu almak mümkün

```
kubectl get pods podname -o yaml
```

replicaset e alternatifler 
- Deployment (tavsiye edilen)
- Bare Pods
- Job
- DaemonSet
- ReplicationController


### Replication Set

bunun en güzel özelliği fail olan podları replace etmesi, fazla ise silmesi ve eksik ise create etmesi. bu nedenle tek bir node bile olsa bu yöntemi tercih etmek en mantıklısı.


örnek 

```yml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

```
kubectl apply -f https://k8s.io/examples/controllers/replication.yaml

kubectl describe replicationcontrollers/nginx
```

alternatifleri

- Deployment (tavsiye edilen)
- Bare Pods
- Job
- DaemonSet
- replicaset

### Deployments

A Deployment provides declarative updates for Pods and ReplicaSets.

The following are typical use cases for Deployments:

- Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.
- Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.
- Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.
- Scale up the Deployment to facilitate more load.
- Pause the Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.
- Use the status of the Deployment as an indicator that a rollout has stuck.
- Clean up older ReplicaSets that you don’t need anymore.

örnek 

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```


ilgili sayfa layout u bozuk olduğu için daha fazla yazamadım. daha sonra kontrol edilmeli

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

### StatefulSets

Storage larla ilgili güncellemler için bu kullanılır. volume bozulduğunda yenisini bağlayabilir.

StatefulSet ler aşağıdakiler den biri veya birkaçı gerkli olduğunda faydalı bir araçtır.

- Stable, unique network identifiers.
- Stable, persistent storage.
- Ordered, graceful deployment and scaling.
- Ordered, automated rolling updates.


__Limitations__

- The storage for a given Pod must either be provisioned by a PersistentVolume Provisioner based on the requested storage class, or pre-provisioned by an admin.
- Deleting and/or scaling a StatefulSet down will not delete the volumes associated with the StatefulSet. - - This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources.
- StatefulSets currently require a Headless Service to be responsible for the network identity of the Pods. - You are responsible for creating this Service.
- StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the  StatefulSet down to 0 prior to deletion.
- When using Rolling Updates with the default Pod Management Policy (OrderedReady), it’s possible to get into a broken state that requires manual intervention to repair.


örnek

```
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi
```

__pod selector__

statefulSet speciselctor altında .spec.template.metadata.labels ile eşleşen bir labela eşleşmeli

yukarıdaki örnekteki gibi
```
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
```


__update strategy__

- On Delete: geriyeuıyumlulk için kulanıllıyor. bu .spec.updateStrategy.type de yarlandığında statwefulSet otomatik olarak podlar ıupdate etmerz. silme işlemi manule yapılmalıdır / yaplır.
- Rolling Upddate: defaul da .spec.updateStrategy unpecifiied olarak ayarlıdır. .spec.updateStrategy.type RollingUpdate olarka ayarlanırsa StatfulSet podları recreate edecektir.

### DaemonSet

bazı pod lar vardır ki her bir node a çalışması gerekir. node eklendiğinde podlar da otomatik olarak eklenir. örnek bazı deamonset ler

- running a cluster storage daemon, such as glusterd, ceph, on each node.
- running a logs collection daemon on every node, such as fluentd or filebeat.
- running a node monitoring daemon on every node, such as Prometheus Node Exporter, Flowmill, Sysdig Agent, collectd, Dynatrace OneAgent, AppDynamics Agent, Datadog agent, New Relic agent, Ganglia gmond, Instana Agent or Elastic Metricbeat.

örnek 

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

```
kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
```


A DaemonSet ensures that all eligible nodes run a copy of a Pod. Normally, the node that a Pod runs on is selected by the Kubernetes scheduler. However, DaemonSet pods are created and scheduled by the DaemonSet controller instead. That introduces the following issues:

__Alternatives to DaemonSet__

- Init Script
- Bare Pods
- Static Pods
- Deployments


### Garbege Collection

bakılabilir : https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/

### TTL Controller for Finished Resources

bakılabilir: https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/

### Jobs - Run to Completion

bir ve ya daha fazla pod un görevini bitirdikten sonra yok edildiğinden emin olmak için yazıla joblardır.

örnek 

```
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4
```
örenek iş bittikten 100 saniye sonra job silinecek

```
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-ttl
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```


### CronJob

cron table olark verilen schedule a uygun şekilde periyodik olarak çalışır.

örnek

```
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
```
